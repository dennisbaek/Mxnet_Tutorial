import mxnet as mxdef VGG19():    #vgg19 - convolution part    data = mx.sym.Variable("data")    conv1_1 = mx.symbol.Convolution(name='conv1_1', data=data , num_filter=64, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu1_1 = mx.symbol.Activation(name='relu1_1', data=conv1_1 , act_type='relu')    conv1_2 = mx.symbol.Convolution(name='conv1_2', data=relu1_1 , num_filter=64, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu1_2 = mx.symbol.Activation(name='relu1_2', data=conv1_2 , act_type='relu')    pool1 = mx.symbol.Pooling(name='pool1', data=relu1_2 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg')    conv2_1 = mx.symbol.Convolution(name='conv2_1', data=pool1 , num_filter=128, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu2_1 = mx.symbol.Activation(name='relu2_1', data=conv2_1 , act_type='relu')    conv2_2 = mx.symbol.Convolution(name='conv2_2', data=relu2_1 , num_filter=128, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu2_2 = mx.symbol.Activation(name='relu2_2', data=conv2_2 , act_type='relu')    pool2 = mx.symbol.Pooling(name='pool2', data=relu2_2 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg')    conv3_1 = mx.symbol.Convolution(name='conv3_1', data=pool2 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu3_1 = mx.symbol.Activation(name='relu3_1', data=conv3_1 , act_type='relu')    conv3_2 = mx.symbol.Convolution(name='conv3_2', data=relu3_1 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu3_2 = mx.symbol.Activation(name='relu3_2', data=conv3_2 , act_type='relu')    conv3_3 = mx.symbol.Convolution(name='conv3_3', data=relu3_2 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu3_3 = mx.symbol.Activation(name='relu3_3', data=conv3_3 , act_type='relu')    conv3_4 = mx.symbol.Convolution(name='conv3_4', data=relu3_3 , num_filter=256, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu3_4 = mx.symbol.Activation(name='relu3_4', data=conv3_4 , act_type='relu')    pool3 = mx.symbol.Pooling(name='pool3', data=relu3_4 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg')    conv4_1 = mx.symbol.Convolution(name='conv4_1', data=pool3 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu4_1 = mx.symbol.Activation(name='relu4_1', data=conv4_1 , act_type='relu')    conv4_2 = mx.symbol.Convolution(name='conv4_2', data=relu4_1 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu4_2 = mx.symbol.Activation(name='relu4_2', data=conv4_2 , act_type='relu')    conv4_3 = mx.symbol.Convolution(name='conv4_3', data=relu4_2 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu4_3 = mx.symbol.Activation(name='relu4_3', data=conv4_3 , act_type='relu')    conv4_4 = mx.symbol.Convolution(name='conv4_4', data=relu4_3 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu4_4 = mx.symbol.Activation(name='relu4_4', data=conv4_4 , act_type='relu')    pool4 = mx.symbol.Pooling(name='pool4', data=relu4_4 , pad=(0,0), kernel=(2,2), stride=(2,2), pool_type='avg')    conv5_1 = mx.symbol.Convolution(name='conv5_1', data=pool4 , num_filter=512, pad=(1,1), kernel=(3,3), stride=(1,1), no_bias=False)    relu5_1 = mx.symbol.Activation(name='relu5_1', data=conv5_1 , act_type='relu')    # style and content layers selection    style = mx.sym.Group([relu1_1, relu2_1, relu3_1, relu4_1, relu5_1])    content = mx.sym.Group([relu4_2])    return style, contentdef Get_Content_Loss(content_image=None, noise_image=None ,image_size=(), ctx=mx.gpu(0)):    #How to get pretrained model from mxnet 'symbol' - VGG19    style, content = VGG19()    out = content    arg_names = out.list_arguments()    arg_shapes, output_shapes, aux_shapes = out.infer_shape(data=(1, 3, image_size[0], image_size[1]))    #for bind function    args = dict(zip(arg_names, [mx.nd.zeros(shape, ctx=ctx) for shape in arg_shapes]))    args["data"]=image    #vgg19    pretrained = mx.nd.load("vgg19-0000.params")    for name in arg_names:        if name == "data":            continue        key = "arg:" + name        if key in pretrained:            #Copy the weight of the pretrained model.            pretrained[key].copyto(args[name])        else:            print("Skip argument %s" % name)    #dict(data=args["data"]) : It is the target to be differentiated.    '''    args_grad (list of NDArray or dict of str to NDArray, optional) â€“    When specified, args_grad provides NDArrays to hold the result of gradient value in backward.    If the input type is a list of NDArray, the order should be same as the order of list_arguments().    If the input type is a dict of str to NDArray, then it maps the name of arguments to the corresponding NDArray.    When the type is a dict of str to NDArray, one only need to provide the dict for required argument gradient. Only the specified argument gradient will be calculated.    '''    return out.bind(ctx=ctx, args=args, args_grad=dict(data=args["data"]), grad_req="write") , content_graddef Get_Style_Loss(style_image=None, noise_image=None, image_size=(), ctx=mx.gpu(0)):    #weighting_factors=nd.divide(nd.ones((5),ctx=ctx),5)    # How to get pretrained model from mxnet 'symbol' - VGG19    style, content = VGG19()    out = style    arg_names = out.list_arguments()    arg_shapes, output_shapes, aux_shapes = out.infer_shape(data=(1, 3, image_size[0], image_size[1]))    # for bind function    args = dict(zip(arg_names, [mx.nd.zeros(shape, ctx=ctx) for shape in arg_shapes]))    args["data"] = style_imagei    # vgg19    pretrained = mx.nd.load("vgg19-0000.params")    for name in arg_names:        if name == "data":            continue        key = "arg:" + name        if key in pretrained:            # Copy the weight of the pretrained model.            pretrained[key].copyto(args[name])        else:            print("Skip argument %s" % name)    '''    args_grad : list of NDArray or dict of str to `NDArray`, optional    When specified, `args_grad` provides NDArrays to hold    the result of gradient value in backward.            - If the input type is a list of `NDArray`, the order should be same as the order              of `list_arguments()`.            - If the input type is a dict of str to `NDArray`, then it maps the name of arguments              to the corresponding NDArray.            - When the type is a dict of str to `NDArray`, one only need to provide the dict              for required argument gradient.              Only the specified argument gradient will be calculated.    '''    # dict(data=args["data"]) : It is the target to be differentiated.    return out.bind(ctx=ctx, args=args, args_grad=dict(data=args["data"]), grad_req="write")