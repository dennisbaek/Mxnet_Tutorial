import numpy as npimport mxnet as mximport mxnet.ndarray as ndimport mxnet.gluon as gluonimport mxnet.autograd as autogradfrom tqdm import *import osdef transform(data, label):    return data.astype(np.float32)/255, label.astype(np.float32)#MNIST datasetdef FashionMNIST(batch_size):    #transform = lambda data, label: (data.astype(np.float32) / 255.0 , label) # data normalization    train_data = gluon.data.DataLoader(gluon.data.vision.FashionMNIST(root="FashionMNIST" , train = True , transform = transform) , batch_size , shuffle=True , last_batch="rollover") #Loads data from a dataset and returns mini-batches of data.    test_data = gluon.data.DataLoader(gluon.data.vision.FashionMNIST(root="FashionMNIST", train = False , transform = transform) ,128 , shuffle=False, last_batch="rollover") #Loads data from a dataset and returns mini-batches of data.    return train_data , test_data#evaluate the datadef evaluate_accuracy(test_data, time_step, num_inputs, num_hidden, RNN_Cell, ctx):    numerator = 0    denominator = 0    for data, label in test_data:        data = data.as_in_context(ctx).reshape((time_step, -1, num_inputs))        states = nd.zeros(shape=(data.shape[1], num_hidden), ctx=ctx)        label = label.as_in_context(ctx)        outputs, states = RNN_Cell(data, states)        predictions = nd.argmax(outputs[-1], axis=1) #(batch_size,)        predictions = predictions.asnumpy()        label=label.asnumpy()        numerator += sum(predictions == label)        denominator += predictions.shape[0]    return (numerator / denominator)def RNN(epoch = 100 , batch_size=100, save_period=100 , load_period=100 ,learning_rate= 0.1, ctx=mx.gpu(0)):    train_data , test_data = FashionMNIST(batch_size)    #network parameter    time_step = 28    num_inputs = 28    num_hidden = 100    num_outputs = 10    path = "weights/FashionMNIST_weights-{}".format(load_period)    if os.path.exists(path):        print("loading weights")        [wxh, whh, bh , why, by] = nd.load(path)  # weights load        wxh=wxh.as_in_context(ctx)        whh=whh.as_in_context(ctx)        bh=bh.as_in_context(ctx)        why = why.as_in_context(ctx)        by = by.as_in_context(ctx)        params = [wxh , whh , bh , why , by]    else:        print("initializing weights")        wxh = nd.random_normal(loc=0,scale=0.1,shape=(num_hidden , num_inputs) , ctx=ctx)        whh = nd.random_normal(loc=0,scale=0.1,shape=(num_hidden , num_hidden) ,ctx=ctx)        bh = nd.random_normal(loc=0,scale=0.1,shape=(num_hidden,) , ctx=ctx)        why = nd.random_normal(loc=0,scale=0.1,shape=(num_outputs , num_hidden),ctx=ctx)        by = nd.random_normal(loc=0,scale=0.1,shape=(num_outputs,) , ctx=ctx)        params = [wxh , whh , bh , why , by]    # attach gradient!!!    for param in params:        param.attach_grad()    #Fully Neural Network with 1 Hidden layer    def RNN_Cell(input, state, act_type="tanh"):        outputs=[]        h=state        for x in input:            argument1 = nd.FullyConnected(x, weight=wxh, num_hidden=num_hidden, no_bias=True)            argument2 = nd.FullyConnected(h, weight=whh, num_hidden=num_hidden, no_bias=True)            h = nd.Activation(argument1+argument2+bh, act_type=act_type)            output = nd.FullyConnected(data=h, weight=why, bias=by, num_hidden=num_outputs)            output = nd.softmax(data=output)            outputs.append(output)        #print(output)        return outputs , h    def cross_entropy(output, label):        return - nd.sum(label * nd.log(output), axis=0 , exclude=True)    #Adam optimizer    state=[]    optimizer=mx.optimizer.Adam(rescale_grad=1,learning_rate=learning_rate)    for param in params:        state.append(optimizer.create_state(0,param))    for i in tqdm(range(1,epoch+1,1)):        states = nd.zeros(shape=(batch_size, num_hidden), ctx=ctx)        for data,label in train_data:            data = data.as_in_context(ctx).reshape((time_step,-1,num_inputs))            label = label.as_in_context(ctx)            label = nd.one_hot(label , num_outputs)            #states = nd.zeros(shape=(data.shape[1],num_hidden),ctx=ctx)            with autograd.record():                outputs, states = RNN_Cell(data,states)                loss = cross_entropy(outputs[-1],label) # (batch_size,)            loss.backward()            cost = nd.mean(loss).asscalar()            for j,param in enumerate(params):                optimizer.update(0,param,param.grad,state[j])        test_accuracy = evaluate_accuracy(test_data, time_step, num_inputs, num_hidden, RNN_Cell, ctx)        print(" epoch : {} , last batch cost : {}".format(i,cost))        print("Test_acc : {}".format(test_accuracy))        #weight_save        if i % save_period==0:            if not os.path.exists("weights"):                os.makedirs("weights")            print("saving weights")            nd.save("weights/FashionMNIST_weights-{}".format(i),params)    test_accuracy = evaluate_accuracy(test_data, time_step, num_inputs, num_hidden, RNN_Cell, ctx)    print("Test_acc : {}".format(test_accuracy))    return "optimization completed"if __name__ == "__main__":    RNN(epoch=100, batch_size=128, save_period=100 , load_period=100 ,learning_rate=0.001, ctx=mx.gpu(0))else :    print("Imported")